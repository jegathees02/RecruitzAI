{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "from IPython import get_ipython\n",
    "import os\n",
    "import base64\n",
    "from pyngrok import ngrok\n",
    "import random\n",
    "import librosa\n",
    "# import eye_tracking\n",
    "# import voice\n",
    "# import voice_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<flask_cors.extension.CORS at 0x7f2a905271c0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "CORS(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPLOAD_FOLDER = '/home/jegathees5555/Documents/projects/AI_Interview_recruitz/backend/server/'\n",
    "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_video(video_data):        \n",
    "   \n",
    "\n",
    "    if video_data:\n",
    "        video_name = 'uploaded_video.webm'\n",
    "        video_path = os.path.join(app.config['UPLOAD_FOLDER'], video_name)\n",
    "        print(video_path)\n",
    "        print(video_name)\n",
    "        video_data.save(video_path)  # Save the file\n",
    "        \n",
    "        \n",
    "        \n",
    "        # return result\n",
    "        return jsonify({'message': 'Video uploaded successfully'}), 200\n",
    "    else:\n",
    "        return jsonify({'error': 'No video data provided'}), 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-26 18:34:44.395537: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-26 18:34:44.400241: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-26 18:34:44.504066: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-26 18:34:44.505325: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-26 18:34:46.026894: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import cv2\n",
    "import numpy as np\n",
    "from flask import jsonify\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model = load_model('/home/jegathees5555/Documents/projects/AI_Interview_recruitz/backend/facial_emotion/imageclassifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_tracking_method(video_path):\n",
    "    # eye_detector = cv2.CascadeClassifier(\"/home/jegathees5555/Documents/recruitz/backend/eye_track/haarcascade_eye.xml\")\n",
    "    cascade_classifiers = [\n",
    "        cv2.CascadeClassifier(\"/home/jegathees5555/Documents/recruitz/backend/eye_track/haarcascade_eye.xml\"),\n",
    "        cv2.CascadeClassifier(\"/home/jegathees5555/Documents/projects/AI_Interview_recruitz/backend/eye_track/haarcascade_frontalface_default.xml\")\n",
    "        # Add your second Haar Cascade classifier here if needed\n",
    "    ]\n",
    "\n",
    "    def process_frame(frame, eye_detector):\n",
    "        # Convert the frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect the eyes in the frame\n",
    "        eyes = eye_detector.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        # Find the center of the eyes\n",
    "        eye_centers = []\n",
    "        for eye in eyes:\n",
    "            eye_center = (eye[0] + eye[2] // 2, eye[1] + eye[3] // 2)\n",
    "            eye_centers.append(eye_center)\n",
    "\n",
    "        if len(eye_centers) >= 2:\n",
    "            # Calculate the distance between the eyes\n",
    "            eye_distance = np.linalg.norm(np.array(eye_centers[0]) - np.array(eye_centers[1]))\n",
    "\n",
    "            # Calculate the percentage of how much the user is looking into the camera\n",
    "            gaze_percentage = eye_distance / (frame.shape[1] // 2)\n",
    "        else:\n",
    "            gaze_percentage = -1  # Default value if eyes are not detected\n",
    "\n",
    "        return gaze_percentage\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    gaze_percentages = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=len(cascade_classifiers)) as executor:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Process each frame with different Haar Cascade classifiers in parallel\n",
    "            results = list(executor.map(process_frame, [frame] * len(cascade_classifiers), cascade_classifiers))\n",
    "\n",
    "            # Use the results as needed (e.g., take the average)\n",
    "            average_gaze = sum(results) / len(results)\n",
    "            gaze_percentages.append(average_gaze)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if gaze_percentages:\n",
    "        final_output = (sum(gaze_percentages) / len(gaze_percentages)) * 100 * -1\n",
    "        print(f\"Average Gaze Percentage: {final_output:.2f}\")\n",
    "        return final_output\n",
    "    else:\n",
    "        print(\"No gaze data available.\")\n",
    "        return jsonify({\"error\": \"No gaze data available.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_happiness(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Check if the camera is opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "    total_happiness_percentage = 0\n",
    "    total_sadness_percentage = 0\n",
    "    frame_count = 0\n",
    "    gaze_percentage = eye_tracking_method(video_path)\n",
    "    while True:\n",
    "        # Capture a single frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Break the loop if no more frames are available\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Call your eye tracking function on the entire frame\n",
    "        \n",
    "        if(gaze_percentage < 0 or gaze_percentage == 100):\n",
    "            return 1\n",
    "\n",
    "        # Check if the person is looking (gaze_percentage > threshold, adjust the threshold accordingly)\n",
    "        if gaze_percentage > 0:\n",
    "            # Check if the frame is too dark or empty\n",
    "            if np.mean(frame) < 10:\n",
    "                happiness_percentage = 1\n",
    "                sadness_percentage = 99\n",
    "                return 1\n",
    "            else:\n",
    "                # Resize the frame to match the input size of your emotion classification model\n",
    "                resized_frame = cv2.resize(frame, (256, 256))\n",
    "\n",
    "                # Normalize pixel values\n",
    "                resized_frame = resized_frame.astype('float32') / 255.0\n",
    "\n",
    "                # Expand dimensions to create a batch (assuming the model expects a batch input)\n",
    "                input_image = np.expand_dims(resized_frame, axis=0)\n",
    "\n",
    "                # Make predictions using the emotion classification model\n",
    "                predictions = emotion_model.predict(input_image)\n",
    "\n",
    "                # Interpret the predictions\n",
    "                # happiness_percentage, sadness_percentage = classify_emotion(predictions)\n",
    "\n",
    "                print(f'Happiness Percentage: {happiness_percentage:.2f}%')\n",
    "                print(f'Sadness Percentage: {sadness_percentage:.2f}%')\n",
    "\n",
    "                # Accumulate the happiness and sadness percentages\n",
    "                total_happiness_percentage += happiness_percentage\n",
    "                total_sadness_percentage += sadness_percentage\n",
    "                frame_count += 1\n",
    "\n",
    "    # Release the video capture\n",
    "    cap.release()\n",
    "\n",
    "    if frame_count > 0:\n",
    "        # Calculate the average happiness and sadness percentages\n",
    "        average_happiness = total_happiness_percentage / frame_count\n",
    "        average_sadness = total_sadness_percentage / frame_count\n",
    "\n",
    "        print(f'Average Happiness Percentage: {average_happiness:.2f}%')\n",
    "        print(f'Average Sadness Percentage: {average_sadness:.2f}%')\n",
    "        print(gaze_percentage)  # Fix the format specifier here\n",
    "        average_result = (average_happiness + gaze_percentage) / 2\n",
    "        print(f'Average Result: {average_result:.2f}')\n",
    "        return average_result\n",
    "        # if(gaze_percentage < 0):\n",
    "        #     return 1\n",
    "        # else:\n",
    "        #     return average_result\n",
    "        # print('average'+(average_happiness+gaze_percentage)/2)\n",
    "        # return (average_happiness+gaze_percentage)/2\n",
    "        # Make a decision based on the average happiness\n",
    "        # if average_happiness > 50:\n",
    "        #     print(\"The person is happy on average!\")\n",
    "        # else:\n",
    "        #     print(\"The person is not happy on average.\")\n",
    "    else:\n",
    "        print(\"No frames available for processing.\")\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_clarity():\n",
    "    audio_file = \"/home/jegathees5555/Documents/projects/AI_Interview_recruitz/backend/server/output.mp3\"  # Replace with the path to your audio file\n",
    "    \n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    \n",
    "    # Calculate the spectrogram\n",
    "    spectrogram = np.abs(librosa.stft(y))\n",
    "    # Calculate the spectral centroid\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(S=spectrogram)\n",
    "    \n",
    "    # Calculate the mean of the spectral centroid\n",
    "    mean_centroid = np.mean(spectral_centroid)\n",
    "    \n",
    "    return mean_centroid/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_boldness():\n",
    "    audio_file = \"/home/jegathees5555/Documents/projects/AI_Interview_recruitz/backend/server/output.mp3\"  # Replace with the path to your audio file\n",
    "    \n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    \n",
    "    # Calculate the spectrogram\n",
    "    spectrogram = np.abs(librosa.stft(y))\n",
    "    # Calculate the spectral contrast\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(S=spectrogram)\n",
    "    \n",
    "    # Calculate the mean of the spectral contrast\n",
    "    mean_contrast = np.mean(spectral_contrast)\n",
    "    \n",
    "    return mean_contrast*3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from backend.eye_track.eye_tracking import eye_tracking\n",
    "\n",
    "\n",
    "@app.route('/upload_video_new', methods=['POST'])\n",
    "def upload_frontend_video():\n",
    "    try:\n",
    "         # Remove the assumption of JSON data\n",
    "        video_data = request.files['videoData']\n",
    "        upload_video(video_data)\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "    \n",
    "    try:\n",
    "        result = eye_track_module()\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "    \n",
    "    result_str = str(result)\n",
    "\n",
    "    return jsonify({'message': 'Video uploaded successfully', 'result': result_str}), 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/get_result', methods=['GET'])\n",
    "def getResult():\n",
    "    try:\n",
    "        video_path = '/home/jegathees5555/Documents/projects/AI_Interview_recruitz/backend/server/uploaded_video.webm'\n",
    "        eye_contact = check_happiness(video_path)\n",
    "        eye_contact = round(eye_contact,2)\n",
    "        if(eye_contact == 1) :\n",
    "            with app.app_context():\n",
    "                # if(eye_contact == 1):\n",
    "                return jsonify({ \"eye_contact\": eye_contact , \"boldness\" : 1, \"clarity\" : 1, \"confidence\" : 1, \"overall\" : 1})\n",
    "                # else:\n",
    "                #     return jsonify({ \"eye_contact\": eye_contact , \"boldness\" : boldness, \"clarity\" : clarity, \"confidence\" : confidence, \"overall\" : overall})\n",
    "        # voice.voice_extraction_main()\n",
    "        # voice_quality_analysis = voice_output.voice_quality()\n",
    "        clarity = calculate_clarity()\n",
    "        boldness = calculate_boldness()\n",
    "        clarity = round(clarity,2)\n",
    "        boldness = round(boldness,2)\n",
    "        confidence = (eye_contact+clarity+boldness)/3\n",
    "        confidence = round(confidence,2)\n",
    "        overall = (eye_contact+boldness+clarity+confidence)//4\n",
    "        overall = round(overall,2)\n",
    "        # print(\"clarity :\"+clarity + \"confidence: \"+confidence +\" boldness : \"+boldness +\" eye_contact : \"+eye_contact + \"Overall : \"+overall)\n",
    "        with app.app_context():\n",
    "            return jsonify({ \"eye_contact\": eye_contact , \"boldness\" : boldness, \"clarity\" : clarity, \"confidence\" : confidence, \"overall\" : overall})\n",
    "    except Exception as e:\n",
    "        return \"Error occurred: \" + str(e), 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t=2024-01-26T18:34:48+0530 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/home/jegathees5555/.config/ngrok/ngrok.yml legacy_path=/home/jegathees5555/.ngrok2/ngrok.yml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public URL: https://b613-2409-40f4-38-7cba-1373-967b-ad8c-59bc.ngrok-free.app\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://localhost:5000\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Gaze Percentage: 73.13\n",
      "1/1 [==============================] - 0s 301ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [26/Jan/2024 18:35:22] \"GET /get_result HTTP/1.1\" 500 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Set up ngrok and expose the Flask app\n",
    "    ngrok_tunnel = ngrok.connect(5000)\n",
    "    print('Public URL:', ngrok_tunnel.public_url)\n",
    "    app.run(host='localhost', port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
