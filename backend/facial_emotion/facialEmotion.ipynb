{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "# #     for filename in filenames:\n",
    "# #         print(os.path.join(dirname, \"\"))\n",
    "#     print(dirname)\n",
    "print(\"started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 17:01:42.603775: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-22 17:01:42.605280: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-22 17:01:42.637897: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-22 17:01:42.638861: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-22 17:01:43.472507: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import Precision, Recall, Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 64, (64, 64), './dataset/faceEmotion/train', './dataset/faceEmotion/test')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASS = 4\n",
    "batch_size = 64\n",
    "img_height = 64\n",
    "img_width = 64\n",
    "train_dir = os.path.join(\"./dataset/faceEmotion/train\")\n",
    "test_dir = os.path.join(\"./dataset/faceEmotion/test\")\n",
    "CLASS, batch_size, (img_height, img_width), train_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21107 files belonging to 4 classes.\n",
      "Using 20896 files for training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<_BatchDataset element_spec=(TensorSpec(shape=(None, 64, 64, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf.float32, name=None))>,\n",
       " ['fear', 'happy', 'neutral', 'sad'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "#     color_mode=\"grayscale\",\n",
    "    label_mode=\"categorical\",\n",
    "    interpolation=\"area\",\n",
    "    validation_split=0.01,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)\n",
    "train_ds, train_ds.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 10))\n",
    "# for images, labels in train_ds.take(1):\n",
    "#   for i in range(9):\n",
    "#     ax = plt.subplot(3, 3, i + 1)\n",
    "#     plt.imshow(images[i].numpy().astype(\"uint8\"), cmap=\"gray\")\n",
    "# #     plt.title(train_ds.class_names[labels[i]])\n",
    "#     plt.title(train_ds.class_names[np.argmax(labels[i])])\n",
    "# #     print(np.argmax(labels[i]))\n",
    "#     plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5278 files belonging to 4 classes.\n",
      "Using 5225 files for validation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<_BatchDataset element_spec=(TensorSpec(shape=(None, 64, 64, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf.float32, name=None))>,\n",
       " ['fear', 'happy', 'neutral', 'sad'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "#     color_mode=\"grayscale\",\n",
    "    label_mode=\"categorical\",\n",
    "    interpolation=\"area\",\n",
    "    validation_split=0.99,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)\n",
    "test_ds, test_ds.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 10))\n",
    "# for images, labels in test_ds.take(1):\n",
    "#   for i in range(9):\n",
    "#     ax = plt.subplot(3, 3, i + 1)\n",
    "#     plt.imshow(images[i].numpy().astype(\"uint8\"), cmap=\"gray\")\n",
    "# #     plt.title(train_ds.class_names[labels[i]])\n",
    "#     plt.title(test_ds.class_names[np.argmax(labels[i])])\n",
    "# #     print(np.argmax(labels[i]))\n",
    "#     plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from keras.layers import Dense, Conv2D, Flatten,ReLU , MaxPooling2D, Rescaling, Reshape, Dropout, BatchNormalization, AveragePooling2D, Input, RandomFlip, RandomRotation, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 64, 64, 3)]          0         []                            \n",
      "                                                                                                  \n",
      " random_flip (RandomFlip)    (None, 64, 64, 3)            0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " random_width (RandomWidth)  (None, 64, None, 3)          0         ['random_flip[0][0]']         \n",
      "                                                                                                  \n",
      " random_rotation (RandomRot  (None, 64, None, 3)          0         ['random_width[0][0]']        \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " random_translation (Random  (None, 64, None, 3)          0         ['random_rotation[0][0]']     \n",
      " Translation)                                                                                     \n",
      "                                                                                                  \n",
      " random_contrast (RandomCon  (None, 64, None, 3)          0         ['random_translation[0][0]']  \n",
      " trast)                                                                                           \n",
      "                                                                                                  \n",
      " rescaling (Rescaling)       (None, 64, None, 3)          0         ['random_contrast[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 64, None, 10)         280       ['rescaling[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 64, None, 10)         40        ['conv2d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                (None, 64, None, 10)         0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 32, None, 10)         0         ['re_lu[0][0]']               \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 32, None, 32)         2912      ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 32, None, 32)         128       ['conv2d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)              (None, 32, None, 32)         0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 32, None, 32)         9248      ['re_lu_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 32, None, 32)         352       ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 32, None, 32)         128       ['conv2d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 32, None, 32)         128       ['conv2d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 32, None, 32)         0         ['batch_normalization_2[0][0]'\n",
      "                                                                    , 'batch_normalization_3[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)              (None, 32, None, 32)         0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 32, None, 32)         9248      ['re_lu_2[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 32, None, 32)         128       ['conv2d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)              (None, 32, None, 32)         0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 32, None, 32)         9248      ['re_lu_3[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 32, None, 32)         128       ['conv2d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 32, None, 32)         0         ['batch_normalization_5[0][0]'\n",
      "                                                                    , 're_lu_2[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)              (None, 32, None, 32)         0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 16, None, 64)         18496     ['re_lu_4[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 16, None, 64)         256       ['conv2d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)              (None, 16, None, 64)         0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 16, None, 64)         36928     ['re_lu_5[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 16, None, 64)         2112      ['re_lu_4[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 16, None, 64)         256       ['conv2d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 16, None, 64)         256       ['conv2d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 16, None, 64)         0         ['batch_normalization_7[0][0]'\n",
      "                                                                    , 'batch_normalization_8[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)              (None, 16, None, 64)         0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 16, None, 64)         36928     ['re_lu_6[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 16, None, 64)         256       ['conv2d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)              (None, 16, None, 64)         0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 16, None, 64)         36928     ['re_lu_7[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 16, None, 64)         256       ['conv2d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 16, None, 64)         0         ['batch_normalization_10[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     're_lu_6[0][0]']             \n",
      "                                                                                                  \n",
      " re_lu_8 (ReLU)              (None, 16, None, 64)         0         ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 8, None, 128)         73856     ['re_lu_8[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 8, None, 128)         512       ['conv2d_11[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_9 (ReLU)              (None, 8, None, 128)         0         ['batch_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 8, None, 128)         147584    ['re_lu_9[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 8, None, 128)         8320      ['re_lu_8[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, 8, None, 128)         512       ['conv2d_12[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 8, None, 128)         512       ['conv2d_13[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 8, None, 128)         0         ['batch_normalization_12[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " re_lu_10 (ReLU)             (None, 8, None, 128)         0         ['add_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 8, None, 128)         147584    ['re_lu_10[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 8, None, 128)         512       ['conv2d_14[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_11 (ReLU)             (None, 8, None, 128)         0         ['batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 8, None, 128)         147584    ['re_lu_11[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 8, None, 128)         512       ['conv2d_15[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 8, None, 128)         0         ['batch_normalization_15[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     're_lu_10[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_12 (ReLU)             (None, 8, None, 128)         0         ['add_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, 4, None, 256)         295168    ['re_lu_12[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, 4, None, 256)         1024      ['conv2d_16[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_13 (ReLU)             (None, 4, None, 256)         0         ['batch_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, 4, None, 256)         590080    ['re_lu_13[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)          (None, 4, None, 256)         33024     ['re_lu_12[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_17 (Ba  (None, 4, None, 256)         1024      ['conv2d_17[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_18 (Ba  (None, 4, None, 256)         1024      ['conv2d_18[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 4, None, 256)         0         ['batch_normalization_17[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " re_lu_14 (ReLU)             (None, 4, None, 256)         0         ['add_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)          (None, 4, None, 256)         590080    ['re_lu_14[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 4, None, 256)         1024      ['conv2d_19[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_15 (ReLU)             (None, 4, None, 256)         0         ['batch_normalization_19[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)          (None, 4, None, 256)         590080    ['re_lu_15[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_20 (Ba  (None, 4, None, 256)         1024      ['conv2d_20[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, 4, None, 256)         0         ['batch_normalization_20[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     're_lu_14[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_16 (ReLU)             (None, 4, None, 256)         0         ['add_7[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling2d (  (None, 256)                  0         ['re_lu_16[0][0]']            \n",
      " GlobalAveragePooling2D)                                                                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 4)                    1028      ['global_average_pooling2d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2796708 (10.67 MB)\n",
      "Trainable params: 2791888 (10.65 MB)\n",
      "Non-trainable params: 4820 (18.83 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def residual_block(x, filters, strides=1):\n",
    "    # Shortcut branch\n",
    "    shortcut = x\n",
    "\n",
    "    # Main branch\n",
    "    x = layers.Conv2D(filters, kernel_size=(3, 3), strides=strides, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Add shortcut branch if strides are different\n",
    "    if strides != 1 or filters != shortcut.shape[-1]:\n",
    "        shortcut = layers.Conv2D(filters, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "inputs = layers.Input(shape=(img_height, img_width, 3))  # Define the input layer\n",
    "x = inputs  # Assign the input to x\n",
    "x = preprocessing.RandomFlip('horizontal') (x) # flip left-to-right\n",
    "x = preprocessing.RandomWidth(factor=0.2) (x) # horizontal stretch\n",
    "x = preprocessing.RandomRotation(factor=0.50) (x)\n",
    "x = preprocessing.RandomTranslation(height_factor=0.1, width_factor=0.50) (x)\n",
    "x = preprocessing.RandomContrast(0.3) (x)  # contrast change by up to 50%\n",
    "x = layers.Rescaling(1./255) (x)\n",
    "\n",
    "# Initial conv layers (modified for smaller input)\n",
    "x = layers.Conv2D(10, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)  # Reduced kernel size and filters\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "# Stack residual blocks (adjust filter counts as needed)\n",
    "x = residual_block(x, filters=32, strides=1)\n",
    "x = residual_block(x, filters=32, strides=1)\n",
    "x = residual_block(x, filters=64, strides=2)\n",
    "x = residual_block(x, filters=64, strides=1)\n",
    "x = residual_block(x, filters=128, strides=2)\n",
    "x = residual_block(x, filters=128, strides=1)\n",
    "x = residual_block(x, filters=256, strides=2)\n",
    "x = residual_block(x, filters=256, strides=1)\n",
    "\n",
    "# Final layers\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "outputs = layers.Dense(CLASS, activation='softmax')(x)  # Adjust output neurons for your task\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=outputs)  # Create the model\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', Precision(), Recall()],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327/327 [==============================] - 106s 310ms/step - loss: 1.3995 - accuracy: 0.3226 - precision: 0.3565 - recall: 0.0231 - val_loss: 1.4023 - val_accuracy: 0.3278 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/260\n",
      "327/327 [==============================] - 102s 311ms/step - loss: 1.3609 - accuracy: 0.3333 - precision: 0.3730 - recall: 0.0078 - val_loss: 1.5440 - val_accuracy: 0.1964 - val_precision: 0.2671 - val_recall: 0.0794\n",
      "Epoch 3/260\n",
      "327/327 [==============================] - 99s 304ms/step - loss: 1.3595 - accuracy: 0.3361 - precision: 0.4056 - recall: 0.0174 - val_loss: 1.4288 - val_accuracy: 0.2488 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 4/260\n",
      "327/327 [==============================] - 99s 303ms/step - loss: 1.3502 - accuracy: 0.3461 - precision: 0.4533 - recall: 0.0144 - val_loss: 1.3788 - val_accuracy: 0.2840 - val_precision: 0.3553 - val_recall: 0.0155\n",
      "Epoch 5/260\n",
      "327/327 [==============================] - 103s 315ms/step - loss: 1.3473 - accuracy: 0.3451 - precision: 0.4916 - recall: 0.0210 - val_loss: 2.3020 - val_accuracy: 0.2113 - val_precision: 0.2154 - val_recall: 0.1874\n",
      "Epoch 6/260\n",
      "327/327 [==============================] - 97s 295ms/step - loss: 1.3403 - accuracy: 0.3545 - precision: 0.5253 - recall: 0.0324 - val_loss: 1.3417 - val_accuracy: 0.3476 - val_precision: 0.6300 - val_recall: 0.0121\n",
      "Epoch 7/260\n",
      "327/327 [==============================] - 95s 292ms/step - loss: 1.3270 - accuracy: 0.3665 - precision: 0.5601 - recall: 0.0589 - val_loss: 1.7676 - val_accuracy: 0.2218 - val_precision: 0.2306 - val_recall: 0.1560\n",
      "Epoch 8/260\n",
      "327/327 [==============================] - 96s 295ms/step - loss: 1.3150 - accuracy: 0.3763 - precision: 0.5876 - recall: 0.0841 - val_loss: 1.2805 - val_accuracy: 0.3872 - val_precision: 0.8917 - val_recall: 0.0536\n",
      "Epoch 9/260\n",
      "327/327 [==============================] - 98s 298ms/step - loss: 1.2871 - accuracy: 0.3934 - precision: 0.6477 - recall: 0.1108 - val_loss: 1.2338 - val_accuracy: 0.4295 - val_precision: 0.7870 - val_recall: 0.1456\n",
      "Epoch 10/260\n",
      "327/327 [==============================] - 97s 298ms/step - loss: 1.2686 - accuracy: 0.4071 - precision: 0.6597 - recall: 0.1284 - val_loss: 1.3719 - val_accuracy: 0.3548 - val_precision: 0.7404 - val_recall: 0.0551\n",
      "Epoch 11/260\n",
      "327/327 [==============================] - 104s 317ms/step - loss: 1.2404 - accuracy: 0.4253 - precision: 0.6927 - recall: 0.1472 - val_loss: 1.5678 - val_accuracy: 0.2226 - val_precision: 0.2454 - val_recall: 0.1238\n",
      "Epoch 12/260\n",
      "327/327 [==============================] - 107s 329ms/step - loss: 1.2186 - accuracy: 0.4442 - precision: 0.6991 - recall: 0.1731 - val_loss: 1.3497 - val_accuracy: 0.3707 - val_precision: 0.4844 - val_recall: 0.1899\n",
      "Epoch 13/260\n",
      "327/327 [==============================] - 103s 315ms/step - loss: 1.1908 - accuracy: 0.4629 - precision: 0.6943 - recall: 0.2041 - val_loss: 1.8412 - val_accuracy: 0.2115 - val_precision: 0.2128 - val_recall: 0.1642\n",
      "Epoch 14/260\n",
      "327/327 [==============================] - 102s 311ms/step - loss: 1.1645 - accuracy: 0.4814 - precision: 0.6976 - recall: 0.2218 - val_loss: 1.4201 - val_accuracy: 0.3166 - val_precision: 0.4593 - val_recall: 0.0810\n",
      "Epoch 15/260\n",
      "327/327 [==============================] - 107s 326ms/step - loss: 1.1410 - accuracy: 0.4901 - precision: 0.7068 - recall: 0.2474 - val_loss: 1.4876 - val_accuracy: 0.2748 - val_precision: 0.3169 - val_recall: 0.0714\n",
      "Epoch 16/260\n",
      "327/327 [==============================] - 105s 320ms/step - loss: 1.1346 - accuracy: 0.4934 - precision: 0.6951 - recall: 0.2568 - val_loss: 1.1350 - val_accuracy: 0.4731 - val_precision: 0.7300 - val_recall: 0.2272\n",
      "Epoch 17/260\n",
      "327/327 [==============================] - 100s 305ms/step - loss: 1.1113 - accuracy: 0.5080 - precision: 0.7019 - recall: 0.2705 - val_loss: 1.0574 - val_accuracy: 0.5294 - val_precision: 0.6807 - val_recall: 0.3399\n",
      "Epoch 18/260\n",
      "327/327 [==============================] - 104s 317ms/step - loss: 1.0924 - accuracy: 0.5195 - precision: 0.7130 - recall: 0.2902 - val_loss: 1.3180 - val_accuracy: 0.3531 - val_precision: 0.4371 - val_recall: 0.1895\n",
      "Epoch 19/260\n",
      "327/327 [==============================] - 104s 317ms/step - loss: 1.0764 - accuracy: 0.5301 - precision: 0.7222 - recall: 0.3078 - val_loss: 1.2627 - val_accuracy: 0.4046 - val_precision: 0.7544 - val_recall: 0.1317\n",
      "Epoch 20/260\n",
      "327/327 [==============================] - 105s 320ms/step - loss: 1.0684 - accuracy: 0.5327 - precision: 0.7124 - recall: 0.3125 - val_loss: 1.2493 - val_accuracy: 0.4524 - val_precision: 0.5099 - val_recall: 0.3678\n",
      "Epoch 21/260\n",
      "327/327 [==============================] - 101s 309ms/step - loss: 1.0518 - accuracy: 0.5418 - precision: 0.7148 - recall: 0.3284 - val_loss: 1.2560 - val_accuracy: 0.4612 - val_precision: 0.5092 - val_recall: 0.3749\n",
      "Epoch 22/260\n",
      "327/327 [==============================] - 102s 313ms/step - loss: 1.0371 - accuracy: 0.5513 - precision: 0.7210 - recall: 0.3446 - val_loss: 1.1488 - val_accuracy: 0.5018 - val_precision: 0.5929 - val_recall: 0.3816\n",
      "Epoch 23/260\n",
      "327/327 [==============================] - 100s 306ms/step - loss: 1.0291 - accuracy: 0.5560 - precision: 0.7222 - recall: 0.3507 - val_loss: 1.0180 - val_accuracy: 0.5489 - val_precision: 0.7157 - val_recall: 0.3416\n",
      "Epoch 24/260\n",
      "327/327 [==============================] - 151s 464ms/step - loss: 1.0208 - accuracy: 0.5581 - precision: 0.7195 - recall: 0.3578 - val_loss: 1.1870 - val_accuracy: 0.4624 - val_precision: 0.5257 - val_recall: 0.3189\n",
      "Epoch 25/260\n",
      "327/327 [==============================] - 164s 503ms/step - loss: 1.0083 - accuracy: 0.5640 - precision: 0.7220 - recall: 0.3761 - val_loss: 1.0470 - val_accuracy: 0.5397 - val_precision: 0.6707 - val_recall: 0.3734\n",
      "Epoch 26/260\n",
      "327/327 [==============================] - 109s 332ms/step - loss: 0.9982 - accuracy: 0.5689 - precision: 0.7271 - recall: 0.3719 - val_loss: 0.9598 - val_accuracy: 0.5914 - val_precision: 0.6855 - val_recall: 0.4748\n",
      "Epoch 27/260\n",
      "327/327 [==============================] - 98s 299ms/step - loss: 0.9859 - accuracy: 0.5732 - precision: 0.7309 - recall: 0.3860 - val_loss: 1.2786 - val_accuracy: 0.3889 - val_precision: 0.5803 - val_recall: 0.2193\n",
      "Epoch 28/260\n",
      "327/327 [==============================] - 98s 301ms/step - loss: 0.9739 - accuracy: 0.5813 - precision: 0.7298 - recall: 0.3944 - val_loss: 1.0114 - val_accuracy: 0.5661 - val_precision: 0.6798 - val_recall: 0.4059\n",
      "Epoch 29/260\n",
      "327/327 [==============================] - 103s 315ms/step - loss: 0.9740 - accuracy: 0.5836 - precision: 0.7227 - recall: 0.4002 - val_loss: 0.9512 - val_accuracy: 0.5943 - val_precision: 0.6856 - val_recall: 0.4762\n",
      "Epoch 30/260\n",
      "327/327 [==============================] - 99s 302ms/step - loss: 0.9680 - accuracy: 0.5871 - precision: 0.7326 - recall: 0.4098 - val_loss: 1.1519 - val_accuracy: 0.5181 - val_precision: 0.7210 - val_recall: 0.3194\n",
      "Epoch 31/260\n",
      "327/327 [==============================] - 96s 294ms/step - loss: 0.9576 - accuracy: 0.5869 - precision: 0.7307 - recall: 0.4119 - val_loss: 0.9090 - val_accuracy: 0.6071 - val_precision: 0.7288 - val_recall: 0.4748\n",
      "Epoch 32/260\n",
      "327/327 [==============================] - 103s 316ms/step - loss: 0.9425 - accuracy: 0.5988 - precision: 0.7360 - recall: 0.4243 - val_loss: 1.0835 - val_accuracy: 0.5233 - val_precision: 0.6238 - val_recall: 0.3958\n",
      "Epoch 33/260\n",
      "327/327 [==============================] - 104s 317ms/step - loss: 0.9452 - accuracy: 0.6004 - precision: 0.7333 - recall: 0.4219 - val_loss: 1.0921 - val_accuracy: 0.5623 - val_precision: 0.6263 - val_recall: 0.4892\n",
      "Epoch 34/260\n",
      "327/327 [==============================] - 106s 323ms/step - loss: 0.9331 - accuracy: 0.5983 - precision: 0.7399 - recall: 0.4294 - val_loss: 0.9108 - val_accuracy: 0.6121 - val_precision: 0.7048 - val_recall: 0.4884\n",
      "Epoch 35/260\n",
      "327/327 [==============================] - 102s 312ms/step - loss: 0.9341 - accuracy: 0.6001 - precision: 0.7411 - recall: 0.4256 - val_loss: 0.9640 - val_accuracy: 0.5969 - val_precision: 0.6834 - val_recall: 0.5014\n",
      "Epoch 36/260\n",
      "327/327 [==============================] - 99s 304ms/step - loss: 0.9291 - accuracy: 0.6048 - precision: 0.7406 - recall: 0.4341 - val_loss: 0.9095 - val_accuracy: 0.6142 - val_precision: 0.7117 - val_recall: 0.4833\n",
      "Epoch 37/260\n",
      "327/327 [==============================] - 104s 319ms/step - loss: 0.9250 - accuracy: 0.6064 - precision: 0.7374 - recall: 0.4367 - val_loss: 0.8788 - val_accuracy: 0.6172 - val_precision: 0.7565 - val_recall: 0.4589\n",
      "Epoch 38/260\n",
      "327/327 [==============================] - 106s 325ms/step - loss: 0.9215 - accuracy: 0.6080 - precision: 0.7408 - recall: 0.4427 - val_loss: 0.9136 - val_accuracy: 0.6226 - val_precision: 0.7038 - val_recall: 0.5043\n",
      "Epoch 39/260\n",
      "327/327 [==============================] - 106s 324ms/step - loss: 0.9115 - accuracy: 0.6171 - precision: 0.7432 - recall: 0.4481 - val_loss: 0.9600 - val_accuracy: 0.5948 - val_precision: 0.6793 - val_recall: 0.4674\n",
      "Epoch 40/260\n",
      "327/327 [==============================] - 104s 318ms/step - loss: 0.9133 - accuracy: 0.6111 - precision: 0.7410 - recall: 0.4488 - val_loss: 1.2630 - val_accuracy: 0.4681 - val_precision: 0.6292 - val_recall: 0.2676\n",
      "Epoch 41/260\n",
      "327/327 [==============================] - 96s 295ms/step - loss: 0.9054 - accuracy: 0.6182 - precision: 0.7426 - recall: 0.4564 - val_loss: 1.0975 - val_accuracy: 0.5144 - val_precision: 0.7293 - val_recall: 0.2873\n",
      "Epoch 42/260\n",
      "327/327 [==============================] - 99s 304ms/step - loss: 0.8984 - accuracy: 0.6192 - precision: 0.7490 - recall: 0.4598 - val_loss: 0.9085 - val_accuracy: 0.6065 - val_precision: 0.7063 - val_recall: 0.4819\n",
      "Epoch 43/260\n",
      "327/327 [==============================] - 107s 328ms/step - loss: 0.8880 - accuracy: 0.6228 - precision: 0.7461 - recall: 0.4669 - val_loss: 0.8821 - val_accuracy: 0.6293 - val_precision: 0.7148 - val_recall: 0.5277\n",
      "Epoch 44/260\n",
      "327/327 [==============================] - 107s 328ms/step - loss: 0.8927 - accuracy: 0.6194 - precision: 0.7491 - recall: 0.4640 - val_loss: 0.9695 - val_accuracy: 0.5864 - val_precision: 0.6731 - val_recall: 0.4674\n",
      "Epoch 45/260\n",
      "327/327 [==============================] - 107s 328ms/step - loss: 0.8918 - accuracy: 0.6237 - precision: 0.7477 - recall: 0.4648 - val_loss: 0.9587 - val_accuracy: 0.5853 - val_precision: 0.7107 - val_recall: 0.4228\n",
      "Epoch 46/260\n",
      "327/327 [==============================] - 114s 347ms/step - loss: 0.8884 - accuracy: 0.6255 - precision: 0.7458 - recall: 0.4688 - val_loss: 0.9291 - val_accuracy: 0.6195 - val_precision: 0.6835 - val_recall: 0.5328\n",
      "Epoch 47/260\n",
      "327/327 [==============================] - 122s 374ms/step - loss: 0.8861 - accuracy: 0.6285 - precision: 0.7483 - recall: 0.4724 - val_loss: 0.8915 - val_accuracy: 0.6224 - val_precision: 0.7438 - val_recall: 0.4628\n",
      "Epoch 48/260\n",
      "327/327 [==============================] - 110s 335ms/step - loss: 0.8729 - accuracy: 0.6330 - precision: 0.7492 - recall: 0.4843 - val_loss: 0.9622 - val_accuracy: 0.6000 - val_precision: 0.7039 - val_recall: 0.4645\n",
      "Epoch 49/260\n",
      "327/327 [==============================] - 106s 323ms/step - loss: 0.8750 - accuracy: 0.6305 - precision: 0.7488 - recall: 0.4753 - val_loss: 0.9010 - val_accuracy: 0.6224 - val_precision: 0.6927 - val_recall: 0.5187\n",
      "Epoch 50/260\n",
      "327/327 [==============================] - 102s 311ms/step - loss: 0.8724 - accuracy: 0.6308 - precision: 0.7521 - recall: 0.4810 - val_loss: 0.9015 - val_accuracy: 0.6126 - val_precision: 0.7213 - val_recall: 0.4869\n",
      "Epoch 51/260\n",
      "327/327 [==============================] - 103s 313ms/step - loss: 0.8688 - accuracy: 0.6338 - precision: 0.7487 - recall: 0.4835 - val_loss: 0.8818 - val_accuracy: 0.6212 - val_precision: 0.7178 - val_recall: 0.5074\n",
      "Epoch 52/260\n",
      "327/327 [==============================] - 103s 315ms/step - loss: 0.8708 - accuracy: 0.6343 - precision: 0.7495 - recall: 0.4856 - val_loss: 0.8732 - val_accuracy: 0.6302 - val_precision: 0.7166 - val_recall: 0.5280\n",
      "Epoch 53/260\n",
      "327/327 [==============================] - 131s 400ms/step - loss: 0.8709 - accuracy: 0.6339 - precision: 0.7457 - recall: 0.4837 - val_loss: 0.8677 - val_accuracy: 0.6281 - val_precision: 0.7221 - val_recall: 0.5261\n",
      "Epoch 54/260\n",
      "327/327 [==============================] - 107s 327ms/step - loss: 0.8538 - accuracy: 0.6405 - precision: 0.7551 - recall: 0.4979 - val_loss: 0.9134 - val_accuracy: 0.6084 - val_precision: 0.7080 - val_recall: 0.4967\n",
      "Epoch 55/260\n",
      "327/327 [==============================] - 115s 350ms/step - loss: 0.8586 - accuracy: 0.6395 - precision: 0.7485 - recall: 0.4977 - val_loss: 0.8945 - val_accuracy: 0.6318 - val_precision: 0.7139 - val_recall: 0.5106\n",
      "Epoch 56/260\n",
      "327/327 [==============================] - 104s 317ms/step - loss: 0.8578 - accuracy: 0.6406 - precision: 0.7514 - recall: 0.4960 - val_loss: 0.8149 - val_accuracy: 0.6563 - val_precision: 0.7413 - val_recall: 0.5445\n",
      "Epoch 57/260\n",
      "327/327 [==============================] - 106s 324ms/step - loss: 0.8567 - accuracy: 0.6403 - precision: 0.7536 - recall: 0.4955 - val_loss: 0.8162 - val_accuracy: 0.6559 - val_precision: 0.7304 - val_recall: 0.5606\n",
      "Epoch 58/260\n",
      "327/327 [==============================] - 104s 319ms/step - loss: 0.8502 - accuracy: 0.6448 - precision: 0.7509 - recall: 0.5018 - val_loss: 0.8376 - val_accuracy: 0.6507 - val_precision: 0.7184 - val_recall: 0.5596\n",
      "Epoch 59/260\n",
      "327/327 [==============================] - 100s 306ms/step - loss: 0.8501 - accuracy: 0.6427 - precision: 0.7516 - recall: 0.5055 - val_loss: 1.0248 - val_accuracy: 0.5678 - val_precision: 0.6275 - val_recall: 0.4553\n",
      "Epoch 60/260\n",
      "327/327 [==============================] - 103s 316ms/step - loss: 0.8476 - accuracy: 0.6445 - precision: 0.7548 - recall: 0.5039 - val_loss: 0.9158 - val_accuracy: 0.6098 - val_precision: 0.6846 - val_recall: 0.5135\n",
      "Epoch 61/260\n",
      "327/327 [==============================] - 99s 302ms/step - loss: 0.8450 - accuracy: 0.6427 - precision: 0.7529 - recall: 0.5077 - val_loss: 0.9582 - val_accuracy: 0.6000 - val_precision: 0.6782 - val_recall: 0.4811\n",
      "Epoch 62/260\n",
      "327/327 [==============================] - 95s 290ms/step - loss: 0.8377 - accuracy: 0.6520 - precision: 0.7567 - recall: 0.5167 - val_loss: 0.8821 - val_accuracy: 0.6270 - val_precision: 0.7130 - val_recall: 0.5179\n",
      "Epoch 63/260\n",
      "327/327 [==============================] - 95s 290ms/step - loss: 0.8375 - accuracy: 0.6487 - precision: 0.7538 - recall: 0.5116 - val_loss: 0.8666 - val_accuracy: 0.6411 - val_precision: 0.6982 - val_recall: 0.5663\n",
      "Epoch 64/260\n",
      "327/327 [==============================] - 99s 303ms/step - loss: 0.8356 - accuracy: 0.6521 - precision: 0.7549 - recall: 0.5165 - val_loss: 0.8806 - val_accuracy: 0.6295 - val_precision: 0.6972 - val_recall: 0.5353\n",
      "Epoch 65/260\n",
      "327/327 [==============================] - 98s 300ms/step - loss: 0.8326 - accuracy: 0.6513 - precision: 0.7571 - recall: 0.5162 - val_loss: 0.9415 - val_accuracy: 0.6067 - val_precision: 0.6765 - val_recall: 0.4944\n",
      "Epoch 66/260\n",
      "327/327 [==============================] - 98s 300ms/step - loss: 0.8301 - accuracy: 0.6542 - precision: 0.7570 - recall: 0.5235 - val_loss: 0.8625 - val_accuracy: 0.6389 - val_precision: 0.7067 - val_recall: 0.5589\n",
      "Epoch 67/260\n",
      "327/327 [==============================] - 103s 314ms/step - loss: 0.8266 - accuracy: 0.6540 - precision: 0.7568 - recall: 0.5193 - val_loss: 0.8285 - val_accuracy: 0.6555 - val_precision: 0.7234 - val_recall: 0.5770\n",
      "Epoch 68/260\n",
      "327/327 [==============================] - 101s 309ms/step - loss: 0.8258 - accuracy: 0.6540 - precision: 0.7601 - recall: 0.5244 - val_loss: 0.8553 - val_accuracy: 0.6402 - val_precision: 0.7133 - val_recall: 0.5443\n",
      "Epoch 69/260\n",
      "327/327 [==============================] - 103s 314ms/step - loss: 0.8198 - accuracy: 0.6520 - precision: 0.7547 - recall: 0.5232 - val_loss: 0.9179 - val_accuracy: 0.6276 - val_precision: 0.6915 - val_recall: 0.5470\n",
      "Epoch 70/260\n",
      "327/327 [==============================] - 312s 955ms/step - loss: 0.8203 - accuracy: 0.6566 - precision: 0.7578 - recall: 0.5258 - val_loss: 0.8545 - val_accuracy: 0.6425 - val_precision: 0.7104 - val_recall: 0.5573\n",
      "Epoch 71/260\n",
      "327/327 [==============================] - 100s 305ms/step - loss: 0.8211 - accuracy: 0.6548 - precision: 0.7580 - recall: 0.5213 - val_loss: 0.8261 - val_accuracy: 0.6595 - val_precision: 0.7301 - val_recall: 0.5789\n",
      "Epoch 72/260\n",
      "327/327 [==============================] - 101s 309ms/step - loss: 0.8218 - accuracy: 0.6529 - precision: 0.7583 - recall: 0.5236 - val_loss: 0.8144 - val_accuracy: 0.6616 - val_precision: 0.7342 - val_recall: 0.5820\n",
      "Epoch 73/260\n",
      "327/327 [==============================] - 100s 307ms/step - loss: 0.8154 - accuracy: 0.6597 - precision: 0.7638 - recall: 0.5304 - val_loss: 0.8374 - val_accuracy: 0.6482 - val_precision: 0.7290 - val_recall: 0.5569\n",
      "Epoch 74/260\n",
      "327/327 [==============================] - 102s 311ms/step - loss: 0.8101 - accuracy: 0.6585 - precision: 0.7618 - recall: 0.5348 - val_loss: 0.8655 - val_accuracy: 0.6392 - val_precision: 0.7042 - val_recall: 0.5571\n",
      "Epoch 75/260\n",
      "327/327 [==============================] - 683s 2s/step - loss: 0.8156 - accuracy: 0.6595 - precision: 0.7605 - recall: 0.5323 - val_loss: 0.8841 - val_accuracy: 0.6314 - val_precision: 0.7111 - val_recall: 0.5257\n",
      "Epoch 76/260\n",
      "327/327 [==============================] - 101s 309ms/step - loss: 0.8129 - accuracy: 0.6611 - precision: 0.7640 - recall: 0.5349 - val_loss: 0.8215 - val_accuracy: 0.6584 - val_precision: 0.7238 - val_recall: 0.5734\n",
      "Epoch 77/260\n",
      "327/327 [==============================] - 102s 311ms/step - loss: 0.8107 - accuracy: 0.6624 - precision: 0.7631 - recall: 0.5372 - val_loss: 0.8326 - val_accuracy: 0.6555 - val_precision: 0.7260 - val_recall: 0.5694\n",
      "Epoch 78/260\n",
      "327/327 [==============================] - 102s 312ms/step - loss: 0.8074 - accuracy: 0.6639 - precision: 0.7623 - recall: 0.5402 - val_loss: 0.8096 - val_accuracy: 0.6643 - val_precision: 0.7317 - val_recall: 0.5897\n",
      "Epoch 79/260\n",
      "327/327 [==============================] - 102s 313ms/step - loss: 0.8025 - accuracy: 0.6669 - precision: 0.7650 - recall: 0.5476 - val_loss: 0.8835 - val_accuracy: 0.6241 - val_precision: 0.6993 - val_recall: 0.5395\n",
      "Epoch 80/260\n",
      "327/327 [==============================] - 105s 321ms/step - loss: 0.8081 - accuracy: 0.6599 - precision: 0.7581 - recall: 0.5390 - val_loss: 0.9357 - val_accuracy: 0.6226 - val_precision: 0.6872 - val_recall: 0.5403\n",
      "Epoch 81/260\n",
      "327/327 [==============================] - 101s 309ms/step - loss: 0.7978 - accuracy: 0.6702 - precision: 0.7618 - recall: 0.5477 - val_loss: 0.8662 - val_accuracy: 0.6366 - val_precision: 0.7095 - val_recall: 0.5445\n",
      "Epoch 82/260\n",
      "327/327 [==============================] - 93s 285ms/step - loss: 0.8036 - accuracy: 0.6662 - precision: 0.7629 - recall: 0.5415 - val_loss: 0.9084 - val_accuracy: 0.6354 - val_precision: 0.6920 - val_recall: 0.5525\n",
      "Epoch 83/260\n",
      "327/327 [==============================] - 94s 289ms/step - loss: 0.8030 - accuracy: 0.6632 - precision: 0.7593 - recall: 0.5411 - val_loss: 0.8587 - val_accuracy: 0.6496 - val_precision: 0.7179 - val_recall: 0.5709\n",
      "Epoch 84/260\n",
      "327/327 [==============================] - 102s 313ms/step - loss: 0.7995 - accuracy: 0.6663 - precision: 0.7662 - recall: 0.5448 - val_loss: 0.8243 - val_accuracy: 0.6601 - val_precision: 0.7244 - val_recall: 0.5789\n",
      "Epoch 85/260\n",
      "327/327 [==============================] - 102s 311ms/step - loss: 0.7903 - accuracy: 0.6696 - precision: 0.7680 - recall: 0.5513 - val_loss: 0.8940 - val_accuracy: 0.6281 - val_precision: 0.7037 - val_recall: 0.5305\n",
      "Epoch 86/260\n",
      "327/327 [==============================] - 102s 313ms/step - loss: 0.7964 - accuracy: 0.6702 - precision: 0.7656 - recall: 0.5450 - val_loss: 0.9549 - val_accuracy: 0.6050 - val_precision: 0.6914 - val_recall: 0.4974\n",
      "Epoch 87/260\n",
      "327/327 [==============================] - 95s 289ms/step - loss: 0.7936 - accuracy: 0.6679 - precision: 0.7653 - recall: 0.5457 - val_loss: 0.9387 - val_accuracy: 0.5935 - val_precision: 0.6893 - val_recall: 0.4802\n",
      "Epoch 88/260\n",
      "327/327 [==============================] - 94s 288ms/step - loss: 0.7888 - accuracy: 0.6739 - precision: 0.7684 - recall: 0.5560 - val_loss: 0.9966 - val_accuracy: 0.5843 - val_precision: 0.6546 - val_recall: 0.4882\n",
      "Epoch 89/260\n",
      "327/327 [==============================] - 95s 289ms/step - loss: 0.7918 - accuracy: 0.6716 - precision: 0.7643 - recall: 0.5522 - val_loss: 0.8738 - val_accuracy: 0.6362 - val_precision: 0.7092 - val_recall: 0.5456\n",
      "Epoch 90/260\n",
      "327/327 [==============================] - 95s 289ms/step - loss: 0.7906 - accuracy: 0.6717 - precision: 0.7697 - recall: 0.5514 - val_loss: 0.8823 - val_accuracy: 0.6369 - val_precision: 0.7033 - val_recall: 0.5627\n",
      "Epoch 91/260\n",
      "327/327 [==============================] - 93s 286ms/step - loss: 0.7908 - accuracy: 0.6746 - precision: 0.7638 - recall: 0.5540 - val_loss: 0.8611 - val_accuracy: 0.6557 - val_precision: 0.7124 - val_recall: 0.5989\n",
      "Epoch 92/260\n",
      "327/327 [==============================] - 93s 283ms/step - loss: 0.7832 - accuracy: 0.6737 - precision: 0.7672 - recall: 0.5635 - val_loss: 0.8477 - val_accuracy: 0.6465 - val_precision: 0.7134 - val_recall: 0.5656\n",
      "Epoch 93/260\n",
      "327/327 [==============================] - 94s 288ms/step - loss: 0.7833 - accuracy: 0.6772 - precision: 0.7721 - recall: 0.5604 - val_loss: 0.8471 - val_accuracy: 0.6553 - val_precision: 0.7220 - val_recall: 0.5680\n",
      "Epoch 94/260\n",
      "327/327 [==============================] - 92s 282ms/step - loss: 0.7890 - accuracy: 0.6728 - precision: 0.7681 - recall: 0.5545 - val_loss: 0.8827 - val_accuracy: 0.6306 - val_precision: 0.7053 - val_recall: 0.5441\n",
      "Epoch 95/260\n",
      "327/327 [==============================] - 94s 286ms/step - loss: 0.7848 - accuracy: 0.6736 - precision: 0.7659 - recall: 0.5571 - val_loss: 0.8440 - val_accuracy: 0.6580 - val_precision: 0.7149 - val_recall: 0.5835\n",
      "Epoch 96/260\n",
      "327/327 [==============================] - 329s 1s/step - loss: 0.7828 - accuracy: 0.6754 - precision: 0.7659 - recall: 0.5607 - val_loss: 0.8543 - val_accuracy: 0.6492 - val_precision: 0.7056 - val_recall: 0.5747\n",
      "Epoch 97/260\n",
      "327/327 [==============================] - 102s 311ms/step - loss: 0.7744 - accuracy: 0.6794 - precision: 0.7712 - recall: 0.5660 - val_loss: 0.8647 - val_accuracy: 0.6364 - val_precision: 0.7078 - val_recall: 0.5499\n",
      "Epoch 98/260\n",
      "327/327 [==============================] - 103s 313ms/step - loss: 0.7771 - accuracy: 0.6802 - precision: 0.7703 - recall: 0.5628 - val_loss: 0.9014 - val_accuracy: 0.6323 - val_precision: 0.6941 - val_recall: 0.5560\n",
      "Epoch 99/260\n",
      "327/327 [==============================] - 100s 307ms/step - loss: 0.7765 - accuracy: 0.6768 - precision: 0.7689 - recall: 0.5638 - val_loss: 0.9067 - val_accuracy: 0.6329 - val_precision: 0.6962 - val_recall: 0.5495\n",
      "Epoch 100/260\n",
      "327/327 [==============================] - 98s 298ms/step - loss: 0.7736 - accuracy: 0.6813 - precision: 0.7687 - recall: 0.5663 - val_loss: 0.8097 - val_accuracy: 0.6664 - val_precision: 0.7310 - val_recall: 0.5841\n",
      "Epoch 101/260\n",
      "327/327 [==============================] - 99s 303ms/step - loss: 0.7721 - accuracy: 0.6805 - precision: 0.7693 - recall: 0.5698 - val_loss: 0.8563 - val_accuracy: 0.6452 - val_precision: 0.7114 - val_recall: 0.5646\n",
      "Epoch 102/260\n",
      "327/327 [==============================] - 95s 291ms/step - loss: 0.7753 - accuracy: 0.6778 - precision: 0.7698 - recall: 0.5665 - val_loss: 0.9410 - val_accuracy: 0.6276 - val_precision: 0.6799 - val_recall: 0.5554\n",
      "Epoch 103/260\n",
      "327/327 [==============================] - 97s 296ms/step - loss: 0.7814 - accuracy: 0.6738 - precision: 0.7647 - recall: 0.5586 - val_loss: 0.8233 - val_accuracy: 0.6576 - val_precision: 0.7208 - val_recall: 0.5832\n",
      "Epoch 104/260\n",
      "327/327 [==============================] - 104s 319ms/step - loss: 0.7707 - accuracy: 0.6782 - precision: 0.7696 - recall: 0.5661 - val_loss: 0.8538 - val_accuracy: 0.6521 - val_precision: 0.7064 - val_recall: 0.5784\n",
      "Epoch 105/260\n",
      "327/327 [==============================] - 110s 336ms/step - loss: 0.7666 - accuracy: 0.6796 - precision: 0.7669 - recall: 0.5714 - val_loss: 0.8885 - val_accuracy: 0.6463 - val_precision: 0.6943 - val_recall: 0.5732\n",
      "Epoch 106/260\n",
      "327/327 [==============================] - 120s 366ms/step - loss: 0.7690 - accuracy: 0.6791 - precision: 0.7685 - recall: 0.5702 - val_loss: 0.8459 - val_accuracy: 0.6503 - val_precision: 0.7182 - val_recall: 0.5613\n",
      "Epoch 107/260\n",
      "327/327 [==============================] - 216s 663ms/step - loss: 0.7672 - accuracy: 0.6833 - precision: 0.7746 - recall: 0.5734 - val_loss: 0.8833 - val_accuracy: 0.6354 - val_precision: 0.6957 - val_recall: 0.5654\n",
      "Epoch 108/260\n",
      "327/327 [==============================] - 117s 358ms/step - loss: 0.7657 - accuracy: 0.6853 - precision: 0.7714 - recall: 0.5761 - val_loss: 0.8546 - val_accuracy: 0.6492 - val_precision: 0.7035 - val_recall: 0.5795\n",
      "Epoch 109/260\n",
      "327/327 [==============================] - 118s 362ms/step - loss: 0.7687 - accuracy: 0.6828 - precision: 0.7709 - recall: 0.5703 - val_loss: 0.8623 - val_accuracy: 0.6446 - val_precision: 0.7110 - val_recall: 0.5654\n",
      "Epoch 110/260\n",
      "327/327 [==============================] - 102s 312ms/step - loss: 0.7641 - accuracy: 0.6854 - precision: 0.7731 - recall: 0.5795 - val_loss: 0.8378 - val_accuracy: 0.6521 - val_precision: 0.7100 - val_recall: 0.5870\n",
      "Epoch 111/260\n",
      "327/327 [==============================] - 95s 290ms/step - loss: 0.7594 - accuracy: 0.6842 - precision: 0.7725 - recall: 0.5775 - val_loss: 0.8021 - val_accuracy: 0.6727 - val_precision: 0.7315 - val_recall: 0.6050\n",
      "Epoch 112/260\n",
      "327/327 [==============================] - 94s 286ms/step - loss: 0.7574 - accuracy: 0.6868 - precision: 0.7748 - recall: 0.5819 - val_loss: 0.8546 - val_accuracy: 0.6528 - val_precision: 0.7217 - val_recall: 0.5767\n",
      "Epoch 113/260\n",
      "327/327 [==============================] - 94s 287ms/step - loss: 0.7611 - accuracy: 0.6835 - precision: 0.7703 - recall: 0.5795 - val_loss: 0.8802 - val_accuracy: 0.6371 - val_precision: 0.6945 - val_recall: 0.5548\n",
      "Epoch 114/260\n",
      "327/327 [==============================] - 101s 310ms/step - loss: 0.7585 - accuracy: 0.6860 - precision: 0.7728 - recall: 0.5807 - val_loss: 0.8922 - val_accuracy: 0.6467 - val_precision: 0.6995 - val_recall: 0.5755\n",
      "Epoch 115/260\n",
      "327/327 [==============================] - 99s 303ms/step - loss: 0.7645 - accuracy: 0.6829 - precision: 0.7679 - recall: 0.5733 - val_loss: 0.8380 - val_accuracy: 0.6632 - val_precision: 0.7189 - val_recall: 0.5843\n",
      "Epoch 116/260\n",
      "327/327 [==============================] - 105s 321ms/step - loss: 0.7579 - accuracy: 0.6885 - precision: 0.7732 - recall: 0.5807 - val_loss: 0.9941 - val_accuracy: 0.5996 - val_precision: 0.6496 - val_recall: 0.5206\n",
      "Epoch 117/260\n",
      "327/327 [==============================] - 98s 300ms/step - loss: 0.7604 - accuracy: 0.6865 - precision: 0.7725 - recall: 0.5780 - val_loss: 0.8746 - val_accuracy: 0.6436 - val_precision: 0.7091 - val_recall: 0.5523\n",
      "Epoch 118/260\n",
      "327/327 [==============================] - 96s 292ms/step - loss: 0.7570 - accuracy: 0.6858 - precision: 0.7736 - recall: 0.5828 - val_loss: 0.8833 - val_accuracy: 0.6456 - val_precision: 0.7054 - val_recall: 0.5696\n",
      "Epoch 119/260\n",
      "327/327 [==============================] - 96s 294ms/step - loss: 0.7582 - accuracy: 0.6836 - precision: 0.7721 - recall: 0.5790 - val_loss: 0.9019 - val_accuracy: 0.6358 - val_precision: 0.6942 - val_recall: 0.5579\n",
      "Epoch 120/260\n",
      "327/327 [==============================] - 98s 299ms/step - loss: 0.7544 - accuracy: 0.6886 - precision: 0.7749 - recall: 0.5868 - val_loss: 0.8693 - val_accuracy: 0.6452 - val_precision: 0.7122 - val_recall: 0.5675\n",
      "Epoch 121/260\n",
      "327/327 [==============================] - 95s 289ms/step - loss: 0.7558 - accuracy: 0.6885 - precision: 0.7723 - recall: 0.5821 - val_loss: 0.8298 - val_accuracy: 0.6700 - val_precision: 0.7175 - val_recall: 0.6119\n",
      "Epoch 122/260\n",
      "327/327 [==============================] - 95s 290ms/step - loss: 0.7500 - accuracy: 0.6888 - precision: 0.7737 - recall: 0.5845 - val_loss: 0.8591 - val_accuracy: 0.6653 - val_precision: 0.7094 - val_recall: 0.6115\n",
      "Epoch 123/260\n",
      "327/327 [==============================] - 95s 292ms/step - loss: 0.7444 - accuracy: 0.6931 - precision: 0.7736 - recall: 0.5918 - val_loss: 0.8650 - val_accuracy: 0.6431 - val_precision: 0.7232 - val_recall: 0.5464\n",
      "Epoch 124/260\n",
      "327/327 [==============================] - 95s 291ms/step - loss: 0.7451 - accuracy: 0.6941 - precision: 0.7768 - recall: 0.5897 - val_loss: 0.8323 - val_accuracy: 0.6574 - val_precision: 0.7091 - val_recall: 0.5933\n",
      "Epoch 125/260\n",
      "327/327 [==============================] - 95s 291ms/step - loss: 0.7479 - accuracy: 0.6927 - precision: 0.7759 - recall: 0.5893 - val_loss: 0.8871 - val_accuracy: 0.6406 - val_precision: 0.7031 - val_recall: 0.5633\n",
      "Epoch 126/260\n",
      "327/327 [==============================] - 95s 292ms/step - loss: 0.7417 - accuracy: 0.6954 - precision: 0.7773 - recall: 0.5965 - val_loss: 1.0218 - val_accuracy: 0.5770 - val_precision: 0.6347 - val_recall: 0.5028\n",
      "Epoch 127/260\n",
      "327/327 [==============================] - 96s 293ms/step - loss: 0.7436 - accuracy: 0.6954 - precision: 0.7750 - recall: 0.5946 - val_loss: 0.8838 - val_accuracy: 0.6511 - val_precision: 0.6988 - val_recall: 0.5879\n",
      "Epoch 128/260\n",
      "327/327 [==============================] - 95s 290ms/step - loss: 0.7476 - accuracy: 0.6917 - precision: 0.7697 - recall: 0.5921 - val_loss: 0.9199 - val_accuracy: 0.6220 - val_precision: 0.6774 - val_recall: 0.5554\n",
      "Epoch 129/260\n",
      "327/327 [==============================] - 94s 288ms/step - loss: 0.7487 - accuracy: 0.6876 - precision: 0.7733 - recall: 0.5892 - val_loss: 0.8234 - val_accuracy: 0.6565 - val_precision: 0.7053 - val_recall: 0.5956\n",
      "Epoch 130/260\n",
      "327/327 [==============================] - 95s 291ms/step - loss: 0.7394 - accuracy: 0.6946 - precision: 0.7787 - recall: 0.5959 - val_loss: 0.9060 - val_accuracy: 0.6339 - val_precision: 0.6933 - val_recall: 0.5598\n",
      "Epoch 131/260\n",
      "327/327 [==============================] - 102s 313ms/step - loss: 0.7385 - accuracy: 0.6951 - precision: 0.7781 - recall: 0.5961 - val_loss: 0.9108 - val_accuracy: 0.6438 - val_precision: 0.6919 - val_recall: 0.5828\n",
      "Epoch 132/260\n",
      "327/327 [==============================] - 100s 304ms/step - loss: 0.7444 - accuracy: 0.6929 - precision: 0.7763 - recall: 0.5926 - val_loss: 0.7987 - val_accuracy: 0.6790 - val_precision: 0.7230 - val_recall: 0.6226\n",
      "Epoch 133/260\n",
      "327/327 [==============================] - 101s 309ms/step - loss: 0.7405 - accuracy: 0.6973 - precision: 0.7759 - recall: 0.5960 - val_loss: 0.8564 - val_accuracy: 0.6589 - val_precision: 0.7075 - val_recall: 0.5941\n",
      "Epoch 134/260\n",
      "327/327 [==============================] - 97s 297ms/step - loss: 0.7438 - accuracy: 0.6910 - precision: 0.7738 - recall: 0.5904 - val_loss: 0.8527 - val_accuracy: 0.6637 - val_precision: 0.7060 - val_recall: 0.6034\n",
      "Epoch 135/260\n",
      "327/327 [==============================] - 101s 309ms/step - loss: 0.7372 - accuracy: 0.6952 - precision: 0.7770 - recall: 0.5991 - val_loss: 0.8799 - val_accuracy: 0.6601 - val_precision: 0.7092 - val_recall: 0.6008\n",
      "Epoch 136/260\n",
      "327/327 [==============================] - 106s 326ms/step - loss: 0.7373 - accuracy: 0.6971 - precision: 0.7796 - recall: 0.5971 - val_loss: 0.8775 - val_accuracy: 0.6423 - val_precision: 0.6983 - val_recall: 0.5678\n",
      "Epoch 137/260\n",
      "327/327 [==============================] - 100s 307ms/step - loss: 0.7361 - accuracy: 0.6938 - precision: 0.7758 - recall: 0.5977 - val_loss: 0.8746 - val_accuracy: 0.6425 - val_precision: 0.7091 - val_recall: 0.5571\n",
      "Epoch 138/260\n",
      "327/327 [==============================] - 96s 294ms/step - loss: 0.7388 - accuracy: 0.6967 - precision: 0.7744 - recall: 0.5981 - val_loss: 0.8425 - val_accuracy: 0.6473 - val_precision: 0.7056 - val_recall: 0.5705\n",
      "Epoch 139/260\n",
      "327/327 [==============================] - 98s 300ms/step - loss: 0.7329 - accuracy: 0.6966 - precision: 0.7799 - recall: 0.6029 - val_loss: 0.8366 - val_accuracy: 0.6681 - val_precision: 0.7176 - val_recall: 0.6000\n",
      "Epoch 140/260\n",
      "327/327 [==============================] - 101s 310ms/step - loss: 0.7354 - accuracy: 0.7005 - precision: 0.7799 - recall: 0.6005 - val_loss: 0.8458 - val_accuracy: 0.6526 - val_precision: 0.7029 - val_recall: 0.5881\n",
      "Epoch 141/260\n",
      "327/327 [==============================] - 102s 312ms/step - loss: 0.7352 - accuracy: 0.6966 - precision: 0.7803 - recall: 0.5999 - val_loss: 0.8567 - val_accuracy: 0.6570 - val_precision: 0.7081 - val_recall: 0.5872\n",
      "Epoch 142/260\n",
      "327/327 [==============================] - 102s 311ms/step - loss: 0.7320 - accuracy: 0.6960 - precision: 0.7773 - recall: 0.5997 - val_loss: 0.8443 - val_accuracy: 0.6624 - val_precision: 0.7190 - val_recall: 0.6017\n",
      "Epoch 143/260\n",
      "327/327 [==============================] - 101s 308ms/step - loss: 0.7307 - accuracy: 0.7006 - precision: 0.7840 - recall: 0.6032 - val_loss: 0.8807 - val_accuracy: 0.6387 - val_precision: 0.6942 - val_recall: 0.5682\n",
      "Epoch 144/260\n",
      "327/327 [==============================] - 103s 316ms/step - loss: 0.7324 - accuracy: 0.6989 - precision: 0.7786 - recall: 0.6018 - val_loss: 0.8459 - val_accuracy: 0.6593 - val_precision: 0.7095 - val_recall: 0.5931\n",
      "Epoch 145/260\n",
      "327/327 [==============================] - 107s 326ms/step - loss: 0.7333 - accuracy: 0.6965 - precision: 0.7736 - recall: 0.6030 - val_loss: 0.8447 - val_accuracy: 0.6551 - val_precision: 0.7080 - val_recall: 0.5899\n",
      "Epoch 146/260\n",
      "327/327 [==============================] - 104s 319ms/step - loss: 0.7312 - accuracy: 0.6994 - precision: 0.7787 - recall: 0.6037 - val_loss: 0.8206 - val_accuracy: 0.6641 - val_precision: 0.7191 - val_recall: 0.5973\n",
      "Epoch 147/260\n",
      "327/327 [==============================] - 108s 331ms/step - loss: 0.7343 - accuracy: 0.6959 - precision: 0.7780 - recall: 0.5999 - val_loss: 0.8584 - val_accuracy: 0.6540 - val_precision: 0.7074 - val_recall: 0.5987\n",
      "Epoch 148/260\n",
      " 23/327 [=>............................] - ETA: 1:42 - loss: 0.7392 - accuracy: 0.7038 - precision: 0.7809 - recall: 0.5958"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m260\u001b[39m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs=260\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=test_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=[EarlyStopping(monitor='val_accuracy', patience=epochs/10, min_delta=0.001, mode='max', restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jegathees5555/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Assuming your model is already defined and trained\n",
    "# (Include the code for model definition, training, etc. before this point)\n",
    "\n",
    "# Save the entire model to a single file\n",
    "model.save('faceConfidence2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 13:46:27.024280: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-22 13:46:27.059840: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-22 13:46:27.375348: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-22 13:46:27.376940: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-22 13:46:28.584291: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not open camera.\n",
      "Error: Could not capture frame.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# from keras.models import load_model\n",
    "\n",
    "# # Load the pre-trained model\n",
    "# loaded_model = load_model('faceConfidence1.h5')  # Replace with the actual path\n",
    "\n",
    "# # Open the default camera (usually the built-in webcam)\n",
    "# cap = cv2.VideoCapture()\n",
    "# img_height = 100\n",
    "# img_width = 100\n",
    "# # Check if the camera is opened successfully\n",
    "# if not cap.isOpened():\n",
    "#     print(\"Error: Could not open camera.\")\n",
    "#     exit()\n",
    "\n",
    "# while True:\n",
    "#     # Capture a single frame\n",
    "#     ret, frame = cap.read()\n",
    "\n",
    "#     # Check if the frame was captured successfully\n",
    "#     if not ret:\n",
    "#         print(\"Error: Could not capture frame.\")\n",
    "#         break\n",
    "\n",
    "#     # Preprocess the frame (resize, normalize, etc.)\n",
    "#     processed_frame = cv2.resize(frame, (img_height, img_width))\n",
    "#     processed_frame = processed_frame / 255.0\n",
    "#     processed_frame = np.expand_dims(processed_frame, axis=0)\n",
    "\n",
    "#     # Make predictions using the loaded model\n",
    "#     predictions = loaded_model.predict(processed_frame)\n",
    "\n",
    "#     # Interpret the predictions (for example, if it's a classification task)\n",
    "#     predicted_class = np.argmax(predictions)\n",
    "\n",
    "#     # Display the frame with prediction information\n",
    "#     cv2.putText(frame, f\"Predicted Class: {predicted_class}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "#     cv2.imshow(\"Real-time Prediction\", frame)\n",
    "\n",
    "#     # Break the loop if the 'q' key is pressed\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # Release the camera\n",
    "# cap.release()\n",
    "\n",
    "# # Close all OpenCV windows\n",
    "# cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
